# POC Simulation (Data Analyst)

# üßæ The Challenge

A large manufacturing company‚Äôs Procurement department is kicking off a digitalization journey. Their category managers have hit a wall ‚Äì they can‚Äôt properly analyze spend because their supplier database is cluttered with messy, duplicate, and outdated entries. Meanwhile, leadership is pushing hard for a clear cost-saving strategy for next year. On top of that, there‚Äôs interest in exploring sustainability in the supply chain, but they just don‚Äôt have the resources to prioritize it right now.

They‚Äôre currently piloting solutions with two competitors: a well-known legacy provider and a newcomer. While they‚Äôre fairly satisfied with the newcomer‚Äôs performance, the legacy player‚Äôs strong market reputation and proven value still carry weight. Budget is already allocated, and they‚Äôre set to make a decision next quarter.

### 1. Entity Resolution

You‚Äôve received a sample of companies from the client for the POC. Each entry has been processed through our entity resolution engine, returning up to 5 candidate matches per row. Your task is to select the best match for each input. If none are accurate, you can leave it unmatched or find the correct entity elsewhere. The goal is to resolve every row to a real-world company‚Äîif it exists online, you should be able to find it.

### 2. Data analysis and QC

Once you‚Äôve picked the correct matches, review the data attributes we provide and look for any inconsistencies. You don‚Äôt need to fix them (though you‚Äôre free to if you want üòÑ), but you should think through how to curate the dataset so it‚Äôs clean and ready for the client.

### 3. Summarize your work

Walk us through everything you did and observed during the project. We care more about understanding your thinking process than the specific tools you used to get there.

### 4. Publish your work

Choose the format that you think best fits this type of challenge / use case and publish it. While solutions to all other challenges are expected to be published on github,  you can use the github field in the form below to link your work, even if it leads to a different domain other than github.

### 5. Submit the challenge

Submit your challenge in the form below.

# üìö Data Flow

## 1. Data Acquisition
The process began by downloading the CSV data sample provided. 
The dataset includes both:
- Input data: values prefixed with input_, representing the client-provided company information.
- Engine-generated data: values without the prefix, generated by Veridion‚Äôs entity resolution engine.

## 2. Data Inspection and Structure Understanding

To analyze the dataset efficiently, I initially opened the CSV file in Excel and I used the ‚ÄúText to Columns‚Äù feature (delimiter: ,) for better readability.

Key Observations:
- Each record group (identified by the input_row_key column) contains up to 5 possible candidate matches. The fact that the dataset has these grouped information made me use SQL in order to deal with this challenge.
- The dataset follows a mirrored structure: each "input_" column has a corresponding field from Veridion. Example: input_company_name <-> company_name, input_main_country_code <-> main_country_code, input_main_city <-> main_city, etc.

## 3. Technology Choice: DuckDB

Given the CSV format and the structured nature of the task, I chose DuckDB for its:
- In-memory querying capabilities;
- Native support for CSV files;
- SQL-first interface that‚Äôs fast and efficient for writing code.

## 4. Data Load into DuckDB
Instead of loading the data via SQL, I used DuckDB‚Äôs ‚ÄúAttach Database‚Äù through ‚ÄúFile‚Äù option to load the CSV file directly. This method allows browsing and querying the CSV file in-place, without requiring a manual CREATE TABLE statement. This approach is especially convenient for quick data exploration and avoids schema declaration, as DuckDB automatically infers column types and structure from the file.

Once loaded, I verified the structure using:

```
SELECT * FROM veridion;
```

This confirmed the presence of both "input_" columns (client data) and their corresponding Veridion-generated fields.

## 5. Matching Strategy: Scoring via CTEs

The goal was to find the best match per each company. I designed a point-based scoring mechanism to evaluate similarity between "input_" data and Veridion-generated data.

Matching Score Logic: Each pair of fields (e.g. input_main_city <-> main_city) contributes to a cumulative score if the values are equal. In other words, the more similar the data ("input_" vs. "Veridion-generated"), the greater the power of a best match.

| Matching Field Pair  | Points |
| ------------- | ------------- |
| input_main_country_code = main_country_code  | 1 |
| + input_main_country = main_country  | +1  |
| + input_main_region = main_region  | +1  |
| + input_main_city = main_city  | +1  |
| + input_main_postcode = main_postcode  | +1  |
| + input_main_street = main_street  | +1  |
| + input_main_street_number = main_street_number  | +1  |
| MAXIMUM TOTAL  | 7  |

## 6. SQL Logic ‚Äì Score and the Best Match

```
-- Step 1: Scoring and attributing pointers in order to find the most appropriate best match (7 = the most powerful relationship and 1 = weak relationship)
WITH scored_matches AS (
  SELECT *,
    CASE
      WHEN input_main_country_code = main_country_code AND
           input_main_country = main_country AND
           input_main_region = main_region AND
           input_main_city = main_city AND
           input_main_postcode = main_postcode AND
           input_main_street = main_street AND
           input_main_street_number = main_street_number THEN 7

      WHEN input_main_country_code = main_country_code AND
           input_main_country = main_country AND
           input_main_region = main_region AND
           input_main_city = main_city AND
           input_main_postcode = main_postcode AND
           input_main_street = main_street THEN 6

      WHEN input_main_country_code = main_country_code AND
           input_main_country = main_country AND
           input_main_region = main_region AND
           input_main_city = main_city AND
           input_main_postcode = main_postcode THEN 5

      WHEN input_main_country_code = main_country_code AND
           input_main_country = main_country AND
           input_main_region = main_region AND
           input_main_city = main_city THEN 4

      WHEN input_main_country_code = main_country_code AND
           input_main_country = main_country AND
           input_main_region = main_region THEN 3

      WHEN input_main_country_code = main_country_code AND
           input_main_country = main_country THEN 2

      WHEN input_main_country_code = main_country_code THEN 1

      ELSE 0
    END AS pointers
  FROM veridion
),

-- Step 2: Rank candidates per input_row_key based on score
ranked_matches AS (
  SELECT *,
    ROW_NUMBER() OVER (
      PARTITION BY input_row_key
      ORDER BY pointers DESC
    ) AS rank
  FROM scored_matches
)

-- Step 3: Select only the top-ranked match for each input
SELECT input_row_key, input_company_name, company_name, pointers, rank
FROM ranked_matches
WHERE rank = 1
ORDER BY input_row_key;
```

## 7. Output & Results

The final output contains 592 rows, representing the top-ranked match for each input data.
This solution ensures that:
- Each input row is matched with the most relevant company from Veridion‚Äôs database;
- The process is fully reproducible and explainable using SQL logic alone.

## 8. Data analysis and QC

At this point, we'll be talking about data cleaning. The purpose of data cleaning is to ensure that a dataset is accurate, consistent, and usable for analysis or client delivery. In our case, the dataset presales_data_sample.csv needs to be curated.

**STEP 1:** Initial Dataset Overview
- Rows: 2,951
- Columns: 76
- Data includes: company details, addresses, contact info, industry codes, descriptions, etc.
- Many columns begin with input_ vs others ‚Äî indicating a comparison between user-provided vs Veridion-generated data.

**STEP 2:** Identify Issues & Inconsistencies
- Missing values in key columns such as year_founded, revenue, and contact information.
- Inconsistent formatting: some URLs contain encoded characters (%20 in twitter_url). Also, phone_numbers are numeric floats instead of strings (can lose leading 0s!). Moreover, there are casing differences in fields like main_country, main_city.
- Data type issues: primary_phone as float is risky, created_at and last_updated_at should be datetime, but are stored as string.

**STEP 3:** Curation Plan Summary


| Issue Type  | Column(s) affected | Suggested Action |
| ------------- | ------------- | ------------ |
| Missing data  | Most columns, especially year_founded, social URLs  | Flag for client, impute, or drop |
| Redundancy  | input_ vs. matched columns  | Clarify purpose, deduplicate |
| Wrong data type  | phone_numbers, dates  | Convert to correct type |
| Inconsistent values  | 	twitter_url, linkedin_url, etc.  | Trim/clean strings |
| Column naming  | Inconsistent naming like naics_2022_primary_code vs sic_codes  | Standardize prefixes or group |





