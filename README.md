# POC Simulation (Data Analyst)

# ğŸ§¾ The Challenge

A large manufacturing companyâ€™s Procurement department is kicking off a digitalization journey. Their category managers have hit a wall â€“ they canâ€™t properly analyze spend because their supplier database is cluttered with messy, duplicate, and outdated entries. Meanwhile, leadership is pushing hard for a clear cost-saving strategy for next year. On top of that, thereâ€™s interest in exploring sustainability in the supply chain, but they just donâ€™t have the resources to prioritize it right now.

Theyâ€™re currently piloting solutions with two competitors: a well-known legacy provider and a newcomer. While theyâ€™re fairly satisfied with the newcomerâ€™s performance, the legacy playerâ€™s strong market reputation and proven value still carry weight. Budget is already allocated, and theyâ€™re set to make a decision next quarter.

### 1. Entity Resolution

Youâ€™ve received a sample of companies from the client for the POC. Each entry has been processed through our entity resolution engine, returning up to 5 candidate matches per row. Your task is to select the best match for each input. If none are accurate, you can leave it unmatched or find the correct entity elsewhere. The goal is to resolve every row to a real-world companyâ€”if it exists online, you should be able to find it.

### 2. Data analysis and QC

Once youâ€™ve picked the correct matches, review the data attributes we provide and look for any inconsistencies. You donâ€™t need to fix them (though youâ€™re free to if you want ğŸ˜„), but you should think through how to curate the dataset so itâ€™s clean and ready for the client.

### 3. Summarize your work

Walk us through everything you did and observed during the project. We care more about understanding your thinking process than the specific tools you used to get there.

### 4. Publish your work

Choose the format that you think best fits this type of challenge / use case and publish it. While solutions to all other challenges are expected to be published on github,  you can use the github field in the form below to link your work, even if it leads to a different domain other than github.

### 5. Submit the challenge

Submit your challenge in the form below.

# ğŸ“š Data Flow

## 1. Data Acquisition
The process began by downloading the CSV data sample provided. 
The dataset includes both:
- Input data: values prefixed with input_, representing the client-provided company information.
- Candidate match data: values without the prefix, generated by Veridionâ€™s entity resolution engine.

## 2. Data Inspection and Structure Understanding

To analyze the dataset efficiently, I initially opened the file in Excel and used the â€œText to Columnsâ€ feature (delimiter: ,) for better readability.

Key Observations:
- Each record group (identified by the input_row_key column) contains up to 5 possible candidate matches. The fact that the dataset has these grouped information made me use SQL in order to deal with this challenge.
- The dataset follows a mirrored structure: each "input_" column has a corresponding field from Veridion. Example: input_company_name <-> company_name, input_main_country_code <-> main_country_code, input_main_city <-> main_city, etc.

## 3. Technology Choice: DuckDB

Given the CSV format and the structured nature of the task, I chose DuckDB for its:
- In-memory querying capabilities;
- Native support for CSV files;
- SQL-first interface thatâ€™s fast and efficient for writing code.

## 4. Data Load into DuckDB
Instead of loading the data via SQL, I used DuckDBâ€™s â€œAttach Databaseâ€ through â€œFileâ€ option to load the CSV file directly. This method allows browsing and querying the CSV file in-place, without requiring a manual CREATE TABLE statement. This approach is especially convenient for quick data exploration and avoids schema declaration, as DuckDB automatically infers column types and structure from the file.

Once loaded, I verified the structure using:

```
SELECT * FROM veridion;
```

This confirmed the presence of both "input_" columns (client data) and their corresponding Veridion-generated fields.

## 5. Matching Strategy: Scoring via CTEs

The goal was to find the best match per each company. I designed a point-based scoring mechanism to evaluate similarity between "input_" data and Veridion generated data.

Matching Score Logic: Each pair of fields (e.g. input_main_city <-> main_city) contributes to a cumulative score if the values are equal.

| Matching Field Pair  | Points |
| ------------- | ------------- |
| input_main_country_code = main_country_code  | 1 |
| + input_main_country = main_country  | +1  |
| + input_main_region = main_region  | +1  |
| + input_main_city = main_city  | +1  |
| + input_main_postcode = main_postcode  | +1  |
| + input_main_street = main_street  | +1  |
| + input_main_street_number = main_street_number  | +1  |
| MAXIMUM TOTAL  | 7  |



